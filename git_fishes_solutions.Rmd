---
title: "Git, GitHub & Ocean Acidification"
author: "Daniel Noble"
date: "`r Sys.Date()`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
options(digits=2)
```

# **Introduction**

This tutorial will draw on the skills you have already learnt on data visualisation, data wrangling and NHST to analyse a data set on ocean acidification effects on fish behaviour. However, we'll do that in the context of some new workflows that adopt Git and GitHub. 

Use this Rmarkdown file as your template to complete the tasks outlined within the instructions. There are questions asked in the instructions and here in the RMarkdown file. Answer these as best as you can to document your steps and thinking. There are also a series of coding exercises. 

Throughout we are also going to learn a bit more about how to format Rmarkdown documents. So, pay close attention to the formatting. We're also going to make use of the `bookdown` package now to render your Rmarkdown because, well, it's quite elegant in many ways. Before you start the tasks install the following packages:

```{r loadpacks, message=FALSE, results='hide'}
# Install a load of packages that we'll use. I'll show you a shortcut that I love to use. Try using the p_load function in the "pacman" package. p_load will execute both the install.packages and library commands in one shot so they only need to be used once to install pacman itself.
#install.packages("pacman", repos = "http://cran.us.r-project.org")
library(pacman)

# Install bookdown for rendering because we'll need this. While we're at it, lets also install /load the tidyverse
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick) # basically just list all the packages you want here

```

# **Task 1**

### Provide the link to your GitHub Repo 

While the tasks are meant to get you familiar with how *GitHub* works you should try and make regular commits as you work your way through. We want to see how often you're using *Git*. Below, within the `()` brackets provide the link to your GitHub Repo. Just cut and paste it so we know what the repo name is and where it is. 

[My GitHub Repository]()

By adding the link in the `()` brackets it will hyperlink "My GitHub Repository". This is how to hyperlink in Rmarkdown documents.

# **Task 2**

This task involves cloning your repository on your computer. Where should we place this repository? While GitHub is somewhat of a cloud storage system because it's keeping record of your commits, files and changes in the cloud it is **not** a backup of your files (note that placing `**` around a word will bold it whereas `*` around a word will italicise it). The lack of *GitHub* being a file backup system is important to recognise! 

>**Question 1**: Why should you not rely on *GitHub* as a backup?

```{r, answer1, echo=TRUE, eval = FALSE}

## We shouldn't rely on GitHub as a backup because if files are not tracked and/or pushed up into the cloud they are not backed up. This means that you can still lose work within a project directory. 

```

I'd usually suggest placing your repo in a cloud storage system like, *OneDrive*, *Dropbox*, or *GoogleDrive*. These will ensure that all your files are backed up and *GitHub* will provide a record of the detailed history. 

Before moving on you might have noticed that the code block above has a name `answer1` and I've added an argument `echo = TRUE`. It's good to label code blocks with useful names to help you debug when rendering of your document goes wrong (when, not IF!). These names need to all be unique. There are a whole bunch of arguments that you can list within code chunks that tell each chunk what to do. You can look to the great [Rmarkdown cheetsheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf) for more information.

>**Question 2**: In the second code block called `answer1` if we change echo = TRUE to echo = FALSE what will happen to the code block `answer1` when you render it?

```{r answer2, echo=TRUE, eval = FALSE}
# Changing echo = FALSE will hide the code block in the html.
```

**Important**: Notice that `eval = FALSE` for many code chunks. When you complete the tasks. Make sure to change these to `TRUE` otherwise you will find your code does not run!

# **Task 3**

Hopefully you have now successfully cloned down your *GitHub* repo. This is your new working directory from which your project will develop. 

Hopefully you also created and/or downloaded the following files and added them to your cloned repo:

1) created a new RStudio project file in your working directory
2) download the `OA_activitydat_20190302_BIOL3207.csv` from Wattle and add this to a `data/` folder in your repo 
3) Add the template Rmarkdown file, `git_fishes_student.Rmd` that was provided on wattle. You'll use this file to work through the tasks and answer questions. 

**STOP**: Before moving on, it's a good time to establish some good *GitHub* practices. You have just did an important step -- you added a bunch of new files. It's time stage these files and commit them so that you have a record. Use *GitHub Desktop* to do this. 

This is also a very good point to make sure that you understand relative path names fo coding. Wait, "relative what?". Your working directory can be considered the 'lowest' level of your project. The starting point, if you will. R needs to know where your files are that you want to load. Many people might be familiar with using the `setwd()` function to tell R where your files are that you need to load. This is problematic because everyone's path to a given working directory is different, and so, your code will not work on others' computers. Even across your own computers!

Lucky, RStudio project files take away the main issue by creating a project file that allows anyone to click on it to open the working directory to the same "spot". Having said that, if you start building structure (i.e., folders) in your working directory or repo than you need to understand how to navigate between folders to load and write files in the places that you want. 

Make sure you have added a `data/` folder to your working directory that you just cloned down. Think about how you would write some code to load in a data file that is within this folder. There are a few useful shortcuts you can use. For example, if you use `.` it means 'start from the current working directory'. That is the RStudio project file location if you click on a RStudio project. If you use `..` it means "start from the directory or folder one step back from the existing working directory".

>**Question 3**: **How would you write the path name to load in a data file from the data folder?**

Write your answer in the code chunk below:

```{r, loaddata, eval = TRUE, echo=TRUE}

path <- "./data/OA_activitydat_20190302_BIOL3207.csv"
  
data <- read_csv(path)

```

Let's say that you added a new data folder that was inside an output folder (i.e., `output/data`) to your working directory. The purpose of this folder is to store and track your cleaned up data frame after you have done all your data wrangling and corrections. 

>**Question 4**: Using the `write_csv` function how would you write the path name to save this file to `output/data`?

```{r, write, eval = FALSE, echo=TRUE, results='hide', message=FALSE}

path <- "./output/data/"

write_csv(data, file = path)

```


# **Task 4**

We're ready for some coding! Lets do some coding and wrangling of the `OA_activitydat_20190302_BIOL3207.csv` data that we will use for the workshop. Note above, the `loaddata` code chunk should have already written the code to load the data file. At this point, we're assuming it's in your working space. If not, make sure you run the `loaddata` code chunk. 

First, write some code below to remove missing data

```{r, rmMiss}
# Code to removing missing data from the `OA_activitydat_20190302_BIOL3207.csv` data frame. 
data_clean <- data %>% filter(!is.na(activity))

```


Second, drop out irrelevant columns and check that there are no spelling issues in `species` and `treatment`. Create a table of summary data that includes: 1) the mean, 2) the standard error and 3) the sample sizes of unique fish across ALL six fish species for each treatment. This will also help you detect any spelling errors. You can use the R package [`flextable`](https://ardata-fr.github.io/flextable-book/) to print out the results in a nice neat format! If you haven't heard about `flextable` spend 10 or so minutes having a look at it's functionality. It's incredible! You'll use this for tables throughout.

```{r summaryTab}
# Drop irrelevant columns
data_clean <- data_clean %>% 
              select(-c("...1", "comment", "loc"))

# Check spelling in species and treatment but also generate a summary table
summary_data <- data_clean %>% 
                group_by(species, treatment) %>% 
                summarise(mean = mean(activity), 
                          se=sd(activity) / sqrt(length(unique(animal_id))), 
                          ids = length(unique(animal_id)), 
                          n = n()) %>% 
                rename("Species"       = species,
                       "Treatment"     = treatment,
                       "Mean Activity" = mean,
                       "Standard Error" = se,
                       "N_inds" = ids,
                       "N" = n)

# Use flextable to render the summary table in a tidy format. Bunch of additional formatting can be done to make it pretty. 
flextable(summary_data) %>% bold(part = "header") %>% align(align = "center", part = "all")
```

**STOP**: Before moving on, it's a good time to establish some good *GitHub* practices. You have just done an important step. It's time to save this file, stage it and commit it so that you have a record. Push these changes up using *GitHub Desktop*. It's important to do this frequently. It's probably not needed after every line of code, but it's good to do this when you have completed an important coding step. Of course, there's no harm in doing it more often. It will provide fine-scale tracking for you!

>**Question 5**: The new version of your Rmarkdown file should now be up on *GitHub*. In the browser window click on your most recent commit. Have a look at the file versioning system. You will notice two files side-by-side. Describe what you notice is being presented online. What do the red and green highlights mean?

```{r answer5, echo=TRUE, eval=FALSE}
# When you click on a commit it will show you how each file has changed from the the most recent past commit to the one that is most recent. Previous files are listed on the left of the scree and newest files on the right. Red highlights show text/code that was removed. Green highlights show text/code that was added to the file. 
```

# **Task 5**

Ignoring figures can be important because they take up quite a lot of space in your *GitHub* repo. For example, huge data files, or figures (e.g., png) can take up a tonne of space. We also might not need to save and track these because we can recreate them with our own code or re-download, process, save and track what we need. 

`.gitignore` files are used to control what *Git* tracks and what it ignores. You should have created a new folder path: `output/figures/`. Write some code now to create a pretty figure using ggplot that shows the difference between control and acidification treatments for each of the fish species in the data set:

```{r, prettyfig, fig.align='center', fig.cap="Mean activity for Control and OA treatments across reef fish"}
# ggplot figure showing mean activity for each treatment (Control, OA) for each species.

# Here is the most advanced example you might use based on the stretch task below. Variants of these are all fine

## Make a cool plot of the data based on species and treatment

# Load in some pictures that are provided first, in case you want to pretty it up! Stretch task!
  chromas <- readPNG("./pics/chromis.png")
whitedams <- readPNG("./pics/whitedams.png")
    lemon <- readPNG("./pics/lemon.png")
   humbug <- readPNG("./pics/humbug.png")
  acantho <- readPNG("./pics/acantho.png")
    ambon <- readPNG("./pics/ambon.png")

# Now lets make a plot, boy this took me a while, and I'm still not happy with it, but it's looking cool!
p1 <- ggplot(data_clean, aes(x =species, y = activity)) +
  geom_violin(aes(fill = treatment, col = treatment), alpha = 0.8) +
  geom_sina(data = . %>% filter(activity >= 25), aes(fill = treatment, col = treatment), colour = "black", alpha=0.2) + # Here's a COOL feature. You can wrangle data directly from data frame to highly something neat, like activities all above and below 25. The '.' is meant to represent the tibble data frame and just like we would manipulate there we can do the same here.
  geom_sina(data = . %>% filter(activity <= 25), aes(fill = treatment, col = treatment), colour = "gray", alpha=0.2) +
  labs(x = "Species", y = "Activity (s)", fill = "Treatment", col = "Treatment") +
  ylim(0, 100) +  # Make axes large as we may want to add in pictures of the species
  geom_errorbar(data = summary_data, aes(x = Species, y = `Mean Activity`, ymin = `Mean Activity`+2*`Standard Error`, ymax = `Mean Activity`-2*`Standard Error`), position=position_dodge2(width = 0.01, padding = 0.5), size = 0.5) +   # We've already created the means and se data so we can feed this new tibble in
  geom_point(data = summary_data, aes(x = Species, y = `Mean Activity`, fill = Treatment, col = Treatment), position = position_dodge(width = 0.9), colour = "black") +
  theme_classic() +
  annotate("text", x = seq(from = 0.75, to = 6.5, by= 0.5), y = 63, label = paste0("n = ", summary_data$N)) +
  scale_fill_manual(breaks = c("CO2", "control"),
                    values=c("#fc9197", "#dde6d5")) +
  scale_color_manual(breaks = c("CO2", "control"),
                  values=c("#fc9197", "#dde6d5")) +
  coord_flip() +
  annotation_raster(chromas, xmin = 2.5, xmax = 3.5, ymin = 75, ymax = 100) +
  annotation_raster(whitedams, xmin =5.5, xmax = 6.5, ymin = 75, ymax = 100) +
  annotation_raster(lemon, xmin =4.5, xmax = 5.5, ymin = 75, ymax = 100) +
  annotation_raster(humbug, xmin =3.5, xmax = 4.5, ymin = 75, ymax = 100) +
  annotation_raster(acantho, xmin =0.5, xmax = 1.5, ymin = 75, ymax = 100) +
  annotation_raster(ambon, xmin =1.6, xmax = 2.5, ymin = 75, ymax = 100)

p1
```

**Stretch Task**: The Clark et al. 2020 paper plots some pretty pictures on their figures. You have access to a folder called "pics/". Add the pics of the difference species from the "pics/" folder to your new plot. Explore the function `annotation_raster()` which might help you achieve this goal.

Note the code chunk used to make the figure. It has a `fig.cap` argument. That means Rmarkdown knows it's a figure and it will allow you to create a figure reference call. In other words, we can refer to our Figure \@ref(fig:prettyfig) by referring to the label for the chunk. This will automatically make a legend for us too (assuming you add one in). The same concept applied to Tables but the legend goes above these.

Now that you have the figure you may also want to write / save it as a separate file. Use `ggsave` function to save the figure(s) to your new `output/figures/`:

```{r, savefig}
# Use ggsave to save the figure
ggsave(plot = p1, filename = "species_trt", dpi = 600, device = "png", path = "./output/figs")
```

>**Question 6**: Given that you have added `output/figures/` to your `.gitignore` file describe what you notice about what you see in *GitHub Desktop*. 

```{r}
## The saved figure is not identified as being tracked in your GitHub Desktop. That's because it is ignored now because our .gitignore file has the path ouput/figures, which means that everything in this folder is not tracked.
```

Last question for this task. I promise! It's important to think very carefully about what you track and ignore.

>**Question 7**: Assume that you added the `pics/` folder to your working directory to plot pictures of each fish on your figure. Do you want to track the files in the pic/ folder on GitHub despite them being .png files? Explain your reasoning. 

```{r, answer6}
## Yes. Remember, if you want collaborators to reproduce your code, and your code requires loading in figures in the pics/ folder than you do in fact want to commit and track the .png files in the pics folder. Otherwise, the output cannot be generated when they pull down your changes. The .png figures that your code generates in the ./output/figs/ folder do not, however, need to be tracked because your code will re-generate these files. 
```

# **Task 6**
This task involves teaming up with a collaborator. Exchange *GitHub* username details and add each other to your repo. Clone each others' repo on to your computer. Re-run their code. Does it work? If not, correct it for them. Explain to them WHY it didn't work. After all, they are right beside you! Think carefully about this. Will it still run on their computer if you have corrected it in a certain way?

Now, lets create a new figure in the code block below that simplifies the one produced in **Task 5**. Instead of all species, lets just plot three of the species (chromis, lemon, and acantho). In the figure code chunk make sure you add the necessary arguments (e.g., `fig.cap`) so that you can refer to Figure \@ref(fig:collabFig).

```{r, collabFig, echo=TRUE, eval=TRUE, fig.align='center', fig.cap="Revised figure from collaborator"}
# You want to make changes to your collaborators figure in Task 5. Maybe you want to create a figure that focuses only on three fish species instead of the 5. More specifically, chromis, lemon, and acantho. Add code here to revise their figure to do that.

# First lets just sub-sample data and summary table
data_clean_sub   <- data_clean %>% filter(species %in% c("chromis", "lemon","acantho"))
summary_data_sub <- summary_data %>% filter(Species %in% c("chromis", "lemon","acantho"))

p2 <- ggplot(data_clean_sub, aes(x =species, y = activity)) +
  geom_violin(aes(fill = treatment, col = treatment), alpha = 0.8) +
  geom_sina(data = . %>% filter(activity >= 25), aes(fill = treatment, col = treatment), colour = "black", alpha=0.2) + # Here's a COOL feature. You can wrangle data directly from data frame to highly something neat, like activities all above and below 25. The '.' is meant to represent the tibble data frame and just like we would manipulate there we can do the same here.
  geom_sina(data = . %>% filter(activity <= 25), aes(fill = treatment, col = treatment), colour = "gray", alpha=0.2) +
  labs(x = "Species", y = "Activity (s)", fill = "Treatment", col = "Treatment") +
  ylim(0, 100) +  # Make axes large as we may want to add in pictures of the species
  geom_errorbar(data = summary_data_sub %>% filter(Species %in% c("chromis", "lemon","acantho")), aes(x = Species, y = `Mean Activity`, ymin = `Mean Activity`+2*`Standard Error`, ymax = `Mean Activity`-2*`Standard Error`), position=position_dodge2(width = 0.01, padding = 0.5), size = 0.5) +   # We've already created the means and se data so we can feed this new tibble in
  geom_point(data = summary_data_sub, aes(x = Species, y = `Mean Activity`, fill = Treatment, col = Treatment), position = position_dodge(width = 0.9), colour = "black") +
  theme_classic() +
  annotate("text", x = c(0.75, 1.25, 1.75, 2.25, 2.75, 3.25), y = 63, label = paste0("n = ", summary_data_sub$N)) +
  scale_fill_manual(breaks = c("CO2", "control"),
                    values=c("#fc9197", "#dde6d5")) +
  scale_color_manual(breaks = c("CO2", "control"),
                  values=c("#fc9197", "#dde6d5")) +
  coord_flip() +
  annotation_raster(chromas, xmin = 1.75, xmax = 2.25, ymin = 75, ymax = 100) +
  annotation_raster(lemon, xmin =2.75, xmax = 3.25, ymin = 75, ymax = 100) +
  annotation_raster(acantho, xmin =0.75, xmax = 1.25, ymin = 75, ymax = 100)

p2


```


# **Task 7**
This task involves creating and resolving conflicts. Conflicts in files are denoted with specific markers. They look like this when you open a file with conflicts.

 <<<<<<<<< HEAD
  
  THIS IS YOUR CODE
  
 ==============
  
  THIS IS YOUR PARTNERS CODE
  
 !>>>>>>>>928731027301723

Resolving is easy. Decide on what changes you think are the best to proceed with and remove conflict markers. 

>**Stretch Task**: Try creating another conflict in the `collabFig` code chunk of the Rmarkdown file. Resolve the conflict again. More practice doing this is always good!

**Task 7 Part 2**

Here the students are mean to fill in the readme file with mode detail. What we want to see here is that the students direct users to the files that are important for reproducing their work. (i.e, To reproduce the workflow first open "git_fishes_student.Rmd"). Then, they should identify the file that is needed. (e.g., To process and clean the data start with "OA_activitydat_20190302_BIOL3207.csv". Clean this code up by first running code chunks XX to XX. If you want to start from the clean code use the cleaned up data called "XX.csv" located in the output/data folder.)

They should also provide the column names that they ended up including along with a description of what they mean. This is already provided so they could just cut and paste this or re-word themselves. 


# **Task 8**
There's not too much to do in this task on the coding front! You just need to create a *GitHub* issue and create a 'To Do' list on *GitHub* for you and your collaborator. 

# **Task 9**
Here, run some statistical tests to determine, for each species, whether the control vs high $CO^2$ treatments differ significantly from each other and in what direction. Provide the difference in means, their 95% confidence intervals, t-statistic, df and p-value. 

>**Stretch Task**: You can of course do this for each species seperately, but those who want a challenge, think about how you might use a loop or even some wrangling methods you have already learnt from the tidyverse to run these tests across all 6 species in a single block of code. If you're familiar with functions, you can even think about writing your own function! All these strategies will avoid having to copy and paste large chunks of code. If you're repeating anything, writing functions and loops are good ways to simplify.

```{r, stats, echo=TRUE, eval=TRUE}
# lots of ways to do this. Probably the easiest from an intuitive perspective is to just write a loop. I'd imagine a bunch of folks will want to do this one step at a time. That's ok, but they'll be copying a tonne of code!

stats <- data.frame()

for(i in 1:length(unique(data_clean$species))){
  # Keep it simple. Lets just first filter the data for each species
  spp_tmp <- data_clean %>% filter(species == unique(data_clean$species)[i])
  
  # Now that we have species lets do, say, a t-test. Store that data like we have in the past
  t_test <- t.test(activity ~ treatment, data = spp_tmp)
  
  # Ok, lets now build our table. We could turn this into a function of course, but keep it here. 
  stats[i,"Species"]      <- unique(data_clean$species)[i]
  stats[i,"Mean C"]       <- t_test$estimate[2]
  stats[i,"Mean OA"]      <- t_test$estimate[1]
  stats[i,"Contrast"]     <- t_test$estimate[1]-t_test$estimate[2]
  stats[i,"L95% CI"]      <- t_test$conf.int [1]
  stats[i,"U95% CI"]      <- t_test$conf.int [2]
  stats[i,"t-statistic"]  <- t_test$statistic
  stats[i,"df"]           <- t_test$parameter
  stats[i,"p-value"]      <- t_test$p.value
  
}

```

```{r, method2, eval=TRUE}

# You could also mix with the tidyverse with some functions. Probably easier to write your own function. We haven't taught them much about this yet, but lets assume some advanced students might be able to do this.There's probably more elegant ways of doing this with the tidyverse

my_stats <- function(data){
  stats <- data.frame()
   
 t_test <-  t.test(activity ~ treatment, data = data)

  stats[1,"Mean C"]       <- t_test$estimate[2]
  stats[1,"Mean OA"]      <- t_test$estimate[1]
  stats[1,"Contrast"]     <- t_test$estimate[1]-t_test$estimate[2]
  stats[1,"L95% CI"]      <- t_test$conf.int [1]
  stats[1,"U95% CI"]      <- t_test$conf.int [2]
  stats[1,"t-statistic"]  <- t_test$statistic
  stats[1,"df"]           <- t_test$parameter
  stats[1,"p-value"]      <- t_test$p.value
  
  return(stats)
}

stats <- data_clean %>% 
         group_by(species) %>% 
         nest() %>% 
         mutate(t_test = lapply(data, function(x) my_stats(x))) %>% 
         select(species, t_test) %>% unnest(cols = 2) %>% data.frame()

```


```{r, table1, echo=TRUE, eval=TRUE, tab.cap = "Summary statistics and results of Weltch's Two Sample T-test comparing Control and Acidification treatments across six reef fish species"}
# Using the resulting object created above, which should be a table with all the summary statistics, t, df and p-value for each species create a table. Note that there is a tab.cap argument in the chunk arguments. Write a caption here. 
flextable(stats) %>% bold(part = "header") %>% align(part = "all", align = "center") %>% 
  set_header_labels(
    species = "Species",
    Mean.C = "Mean Activity (Control)", 
    Mean.OA = "Mean Activity (OA)",
    L95..CI = "Lower 95% CI",
    U95..CI = "Upper 95% CI")
```

Now that you have a table, you can reference it within a document. For example, you can make a call to Table \@ref(tab:table1) by referring to the name of the code chunk. When you knit the html it will create a hyperlink to your table and insert a legend above the table for you. How cool is that!?

> **Question 8**: Pick one of your favorite species and write about the results to a reader. Write in the Rmarkdown file below what 1) the means and mean differences between control and acidification treatment is along with 2) the 95% confidence intervals of the difference

You can write your blurb below this. Before you do that, a few cool features of Rmarkdown. You can actually code in objects so that, when they are rendered they replace the inline code chunk with the result. To give you an example, an inline code chunk is written as follows: `r "add code here"`. When you render the document, whatever you place in the "add code here" section will be rendered. In this case, we are giving a string, so it simply just adds that in the place where the code is, render and see for yourself.  

Moving forward on that you can also add in an object and whatever that objects value is will also be spit out. For example, consider the following: `r x = 10; x`. What will happen? Well, it will place in the value 10 when rendered. Again, if you don't believe me, check for yourself by rendering the document. 

This is all **VERY** cool because that means if your code or data change than you can update your entire report very fast. It's also 100% reproducible. We know exactly where every single value in your report comes from. This is all pretty helpful when you want to check your code and report side-by-side. Now that you have a bit more detail fill in the following:

**Mean activity for the `r stats$species[which(stats$species == "lemon")]` species in the control group was `r stats[which(stats$species == "lemon"), "Mean.C"]` (s / min) compared to the OA treatment group, which was `r stats[which(stats$species == "lemon"),"Mean.OA"]` (s / min). The difference between control and OA treatment means was `r stats[which(stats$species == "lemon"),"Contrast"]` (s/min) (95% CI: `r stats[which(stats$species == "lemon"),"L95..CI"]` to `r stats[which(stats$species == "lemon"),"U95..CI"]`).**

Now, using what you just learnt, describe what the null hypothesis being tested was and provide statistical evidence (t-statistic, df, and p-value) to support your conclusion about whether we can reject the null hypothesis. Again, make sure you use in-line code. Write you answer below:

**We are testing the null hypothesis that there is no differences in the means of the control and OA treatments (i.e., $H_{0}$ : $\mu_{control}$ = $\mu_{OA}$). We do not have enough evidence to reject the null hypothesis that there is no mean differences between Control and OA groups (t = `r stats[which(stats$species == "lemon"),"t.statistic"]`, df = `r stats[which(stats$species == "lemon"),"df"]`, p = `r stats[which(stats$species == "lemon"),"p.value"]`)**

Re-analyse the data for a single species using permutations instead of a t-test. Do your results differ?

>**Stretch Task**:  If you really want a challenge try doing permutation tests for each species. Again, loops, functions or tidyverse (or even combinations might help). 

```{r, stretch2, echo=TRUE, eval=FALSE}
# Single species, lets pick lemon

data_lemon <- data_clean %>% filter(species == "lemon")

perms = 10000
perm_dist <- c()

for(i in 1:perms){
  # First, lets permutate the treatment column. In otherwords, randomise it!
    data_lemon_iter <- data_lemon %>% mutate(treatment = sample(treatment, replace = FALSE))
  
  # Now we have one randomised dataset, lets do a t-test and grab the t-statistic. This is the t-statistic sampling distribution we would expect if there were NO differences between mean activity of the treatments. Remember, we created no differences by randomising the treatment labels.
    perm_dist[i] <- t.test(activity ~ treatment, data = data_lemon_iter)$statistic
}

# Now lets calculate the p-value for lemon. Remember, this is the probability of observing the effect we actually observed under the null hypothesis of the means being equal. We have that t-statistic in the 'stats' table we crated above. 
p_lemon <- sum(abs(perm_dist) >= stats$t.statistic[which(stats$species == "lemon")]) / length(perm_dist)
```

```{r, advancesolution, eval = TRUE}
## Across species, probably easiest(?) (well, intuitively at least) to do following double loop! But beware, it takes a while to run. There are much faster ways to do this, but given what we know the students can do I expect most to kind of converge on this.

      perms = 10000
  perm_dist <- c()
data_spp_2  <- data.frame()

for(j in unique(data_clean$species)){
  for(i in 1:perms){
    # Sub to species  
    data_spp <- data_clean %>% filter(species == j)
  
    # First, lets permutate the treatment column. In other words, randomise it!
      data_spp_perm <- data_spp %>% mutate(treatment = sample(treatment, replace = FALSE))
    
    # Now we have one randomised dataset, lets do a t-test and grab the t-statistic. This is the t-statistic sampling distribution we would expect if there were NO differences between mean activity of the treatments. Remember, we created no differences by randomising the treatment labels.
      perm_dist[i] <- t.test(activity ~ treatment, data = data_spp_perm)$statistic
  }
  
    # Now lets calculate the p-value. Remember, this is the probability of observing the effect we actually observed under the null hypothesis of the means being equal. We have that t-statistic in the 'stats' table we crated above.
                     pos <- which(unique(data_clean$species) == j)
    data_spp_2[pos,"name"] <- j
    data_spp_2[pos, "p"] <- sum(abs(perm_dist) >= abs(stats$t.statistic[which(stats$species == j)])) / length(perm_dist)
}
```

Below. Add a few sentences for the species (or multiple species) you talked about above to describe the permutation results:

  **There is very little difference between the permutation and t-test result for any of the species, as expected. Just to convince you see Table \@ref(tab:permT)**

```{r, permT, tab.cap = "Comparison between t and permutations"}
table <- cbind(data_spp_2, pt = stats$p.value)

flextable(table) %>% set_header_labels(name = "Species",
                                       p    = "p permutation",
                                       pt   = "p t-test")

```

# **Task 10**

This is a stretch task on the use of *GitHub* and the challenges (or maybe lack of challenges) of reproducing others' work. If you finish the above tasks, then, have a crack at this one.

**Solutions: Have a look at the video solutions for creating the webpage. This will be variable and there will be challenges for students to reproduce the same figure. Looking at the script from the paper it's missing imported objects or is at least unclear when or where they are generated to make the figure. There is also lots to improve o in the code. 1) setwd() is used; 2) lots of redundant and repeated code -- write some functions would be better or even looping to reduce the amount of code etc. What I want the students to get experience with is the challenges with reproducing other peoples code. If they see these challenges they are more likely to correct them in their own work**

