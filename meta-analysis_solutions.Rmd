---
title: "Meta-analysis: Multi-level models"
author: "<your name and u number>"
date: "`r Sys.Date()`"
bibliography: ./bib/refs.bib
csl: ./bib/the-journal-of-experimental-biology.csl
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, tidy = TRUE)
options(digits=2)
```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
#install.packages("devtools")
remotes::install_github("rlesur/klippy")
klippy::klippy(tooltip_message = 'Click to Copy Code', tooltip_success = 'Done', position = 'right', color = "red")

# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr)

# To use mathjaxr you need to run equatags::mathjax_install()
```

## **Introduction: Why do we do meta-analysis?**

Meta-analysis is the gold-standard for research synthesis across all disciplines. We synthesise studies to gain broader insights into the efficacy of treatment effects and / or relationships between variables. In biology, we are also dealing with many different populations and species. As such, we're not just interested in understanding 'what the overall effect' actually is (in fact, we may not even care in many cases), but we are mainly focused on attempting to understand what factors (e.g., biological, methodological) explain variation in effects [@Noble2022; @Lag2010; @Gurevitch2018]. In fact, reporting upon measures of variability, or what is referred to as 'heterogeneity' in meta-analysis, is essential to meta-analysis reporting and placing effects within context [@ODea2021; @NakagawaSantos2012; @Borenstein2019; @Nakagawa2017; @Gurevitch2018]. There are a number of important metrics of heterogeneity that are commonly used and reported upon in the meta-analytic literature [@NakagawaSantos2012; @Borenstein2019; @Nakagawa2017]. We'll discuss some of the key ones.

Applying the statistical tools of meta-analysis is therefore fundamental to establishing the generality of findings, testing hypotheses about what might drive variation among effects and identifying publication biases that may result in a skewed picture of existing work. Such insights have the potential to shape future research in important ways [@KortichevaGure2013].

## **How does meta-analysis work?**

Meta-analysis is a way to synthesise [effect sizes](https://daniel1noble.github.io/meta-workshop/effect-size) using a special (or maybe not so special) set of models that account for each effect sizes sampling variance. In other words, meta-analyses are:

> Statistical methods and techniques for aggregating, summarizing, and drawing inferences from collections of studies

<br>
Why would we want to account for a given study's sampling variance? We want to do this because studies vary greatly in their sample size, and thus their power, to detect effects. As meta-analysts we want to weight effect sizes from studies with higher power more in an analysis. These studies are more likely to be 'correct' and their estimates less biased as a result of sampling variance.

[Marc Lajeunesse](http://lajeunesse.myweb.usf.edu) does a brilliant job explaining the goals and types of models that meta-analysts use. In this [short video](https://www.youtube.com/watch?app=desktop&v=3XkC_jetn-U) he describes why weighting in meta-analysis is so important. 

We can effectively think of a meta-analysis as a weighted regression model with the weights being the inverse sampling variance for each effect size. Weights are calculated differently depending on the meta-analytic model in question (more on that in later tutorials). 

Throughout this workshop we will highlight the various features of meta-analysis that we discuss above. We'll try to focus on dissecting how meta-analytic models are working and focus on important aspects of effect sizes and modelling that users should be aware of when meta-analysing a body of work for their specific question of interest. An important aspect to doing meta-analysis well (which can be hard) is being critical of the data and assumptions inherent to both effect sizes and statistical models. Meta-analysis also has some unique sources of non-independence [@Noble2017; @Gurevitch1999; @NakagawaSantos2012; @GleserOlkin2009] that isn't always apparent. We'll try and cover these and discuss some ways in which you can protect yourself from their impacts on inferences. 

## **Meta-analysis with `metafor`!**

Throughout our workshop we will make use of the `metafor` package [@Viechtbauer2010]. It has substantial capabilities. If you're not familiar with `metafor` we would suggest having a look at Wolfgang's fantastic [`UseR` talk]("https://www.youtube.com/watch?v=IkduL5iRdqo"). Having said that, we will go over the various functions as we move through the workshop.

## **Load the necessary R Packages**

```{r loadpacks, message=FALSE, results='hide'}
# Install a load of packages that we'll use. I'll show you a shortcut that I love to use. Try using the p_load function in the "pacman" package. p_load will execute both the install.packages and library commands in one shot so they only need to be used once to install pacman itself.
#install.packages("pacman", repos = "http://cran.us.r-project.org")
library(pacman)

# Install bookdown for rendering because we'll need this. While we're at it, lets also install /load the tidyverse
p_load(bookdown, tidyverse, ggforce, flextable, latex2exp, png, magick, metafor) # basically just list all the packages you want here

```


## **References**

<div id="refs"></div>

<br>
