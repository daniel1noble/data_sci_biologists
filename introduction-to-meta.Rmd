---
title: "Introduction to Meta-analysis"
author: Daniel W.A. Noble
date: '`r Sys.Date()`'
bibliography: ./bib/refs.bib
csl: ./bib/the-journal-of-experimental-biology.csl
output:
  bookdown::html_document2:
    css: style.css
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, tidy = TRUE)
options(digits=2)
```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
#install.packages("devtools")
remotes::install_github("rlesur/klippy")
klippy::klippy(tooltip_message = 'Click to Copy Code', tooltip_success = 'Done', position = 'right', color = "red")

# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr)

# To use mathjaxr you need to run equatags::mathjax_install()
```

## **What is meta-analysis and why do we do it?**

> Statistical methods and techniques for aggregating, summarizing, and drawing inferences from collections of studies

Meta-analysis is the gold-standard for research synthesis across all disciplines. We synthesise studies to gain broader insights into the efficacy of treatment effects and / or relationships between variables. In biology, we are also dealing with many different populations and species. As such, we're not just interested in understanding 'what the overall effect' actually is (in fact, we may not even care in many cases), but we are mainly focused on attempting to understand what factors (e.g., biological, methodological) explain variation in effects [@Noble2022; @Lag2010; @Gurevitch2018]. A key part of meta-analysis is quantifying variability in effects across studies, or what is referred to as 'heterogeneity' in meta-analysis [@ODea2021; @NakagawaSantos2012; @Borenstein2019; @Nakagawa2017; @Gurevitch2018]. 

Think about it this way, in a meta-analysis we want to know what the average effect is across studies to tell us what the mean effect in the population is likely to be. We also want to understand why one experiment gives a different effect size (e.g., Cohen's d) than the same experiment done on a different population. 

## **Meta-analysis aggregates / pools effect sizes**

Meta-analysis is a way to synthesise effect size statistics (i.e., a statistic such as Cohen's *d*) using a special type of mixed effects model that accounts for our sampling uncertainty in each effect estimate. Effect size statistics are the data we use to fit models. 

Effect size statistics can vary but they all have a few key things in common. First, the effect size must be comparable across studies. This means that they must be placed on the same scale so that they can be aggregated. Second, we must be able to estimate the sampling variance for a given effect size. As you're well aware the statistic (effect size) we want to synthesise has uncertainty associated with it because of sampling variability -- the standard error around a mean, for example, is sampling error. In meta-analysis we use this sampling error to decrease the uncertainty in our mean effect size. There are lots of different effect sizes out there, some familiar one are in Table \@ref(tab:tablea). 

```{r tablea, echo = FALSE, tab.cap = "**Common effect sizes used throughout meta-analyses and their associated sampling variances. Examples on when they might be used are also provided**. M: mean; SD: standard deviation; N: sample size"}
FitFlextableToPage <- function(ft, pgwidth = 6){

  ft_out <- ft %>% flextable::autofit()

  ft_out <- flextable::width(ft_out, width = dim(ft_out)$widths*pgwidth /(flextable::flextable_dim(ft_out)$widths))
  return(ft_out)
} 

# I'm just listing a bunch here, but we should think about what we want to include. Here's an example of how we can make a table with all equations. Fun! Really useful website here: https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference. Something wrong with character coding when rendering word document. Has to do with the log odds ratio equation. Renders fine in html, but just stops when word
names <- c("Mean",
           "Log Response Ratio, lnRR",
           # leave this for now??? - we introduce this later
           "Standardised Mean Difference, SMD",
           "Zr (Fisher Transformation of \n Correlation Coefficient, r)")
           #"Log Odds Ratio",
           #"")

# the index i is not really applied consistently so I am adding it more (to do)
# we shoudl discuss what kinds of notations we want to use consistely
           
eqns_yi <- c("M",
             # Senior will have a better version but this is good for now
             "\\ln \\left( \\frac{M_{1}}{M_{2}}\\right)",
             "\\frac{\\left( M_{2} - M_{1}\\right)}{SD_{p}}J",
             # we need to spell out SD_p somewhere
             "\\frac{1}{2}log \\left( \\frac{1+r}{1-r}\\right)")
             #"log\\left(\\frac{y_{i2}+0.5}{n_{i2}-y_{i2}+0.5}\\right) ",
             #"- log\\left(\\frac{y_{i1}+0.5}{n_{i1}-y_{i1}+0.5}\\right)")
             
# Just placeholders for now
eqns_vi <- c("\\frac{SD^2}{N}",
             "\\frac{SD_{1}^2} {M^2_{1}N_{1}} + \\frac{SD_{2}^2} {M^2_{2}N_{2}}",
             "\\frac{N_{1} + N_{2}}{N_{1}N_{2}} + \\frac{SMD^2}{2(N_{1} + N_{2})}",
             # see my commenst
             "\\frac{1}{N - 3}")
             #"\\frac{s_{i}^2}{n_{i}}",
             #"")

# Maybe add some examples for each effect size in a new column. SHould help
eg <- c("CTmin, CTmax, LC50, LT50, Metabolic Rate (MR)",
        "Ratio between pollutant exposed treatment (e.g., BPA-exposed group) and control (no pollutant)",
        "Difference in immune response between males and females, performance difference in the presence of stressor compared to absence of stressor",
        "Relationship between sex hormones and immune responses or metabolic rate and behaviour")

table_fin <- data.frame(names, eqns_yi, eqns_vi, eg)

footnotes <- c("Notes: J = 1 - \\frac{3}{4\\left(N_{1} + N_{2} - 2\\right) - 1}",
               "SD_{p} = \\sqrt \\frac{\\left( N_{1}-1 \\right)SD_{1}^2 + \\left( N_{2}-1 \\right)SD_{2}^2}{N_{1} + N_{2} - 2}")

# Note: It is esssential that you be clear that the function 'compose' is exported from the flextable package. That's because purr also has the same function, so it gets messy.
tablea <- flextable(table_fin)                                                                                %>% 
          flextable::compose(j = 1:4, part = "header", value = as_paragraph(as_b(c("Effect Measure", "Definition", "Sampling Variance", "Examples")))) %>% 
          flextable::compose(j = 2:3, value = as_paragraph(as_equation(c( eqns_yi, eqns_vi))))             %>%
          flextable::compose(j = 4, value = as_paragraph(eg))             %>%
          flextable::width(j = 1, 3)                                                                          %>% 
          flextable::width(j = 2, 2) %>% 
          flextable::width(j = 4, 5) %>% 
          flextable::footnote(i = 4, j = 1, value = as_paragraph(as_equation(footnotes)),
               ref_symbols = c("a"),
               part = "body", inline = TRUE) %>% 
          flextable::font(part = "all", fontname = "Times New Roman")
          
FitFlextableToPage(tablea, pgwidth = 8)
```


To give you a better understanding about how these equations relate to the effect statistics and their sampling error, lets look at the mean in Table \@ref(tab:tablea). We could of course analyse the mean, but it would have to have the same units and meaning across studies. Many studies in ecology and evolution do in fact meta-analyse the mean, but more commonly standardised effect size statistics are used like the log response ratio (big in Ecology), the standardised mean difference (i.e., Cohen's d, Hedges' g -- you already were exposed to Cohen's d in Eric's workshops!), and correlation coefficients.

Let's return to what we learnt at the beginning the semester about sampling distributions. Lets create a loop to calculate the mean of the sample. 

```{r, meansim, class.source='klippy'}
# First lets set up some parameters. We'll take 20 samples from a population with a mean of 10.8 and a sd of 0.988
  nsim = 10000
     n = 20
  mean = 10.8
    sd = 0.988

# Now, lets create an empty vector to store our sample mean. We'll then loop, sampling 20 from the population described above but we'll take 10000 samples so we can get a fairly accurate estimate of the 'true' mean and it's sampling error
  means <- c()

   for (i in 1:nsim){
       sample <- rnorm(n, mean = mean, sd = sd)
        means <- c(means, mean(sample))
    }

```

Let's now have a look at our sampling distribution of means....

```{r sampldist}
# Now lets plot the sampling distribution. We'll show the real, true mean, 10.8, and the actual mean of the sampling distribution.
ggplot2::ggplot(tibble(means), aes(x = means)) + 
  geom_histogram(binwidth = 0.05) + 
  geom_vline(xintercept = 10.8, col = "red") +                     # Add the true mean in red
  geom_vline(xintercept = mean(means), col = "blue", linetype = 2) # Add the mean of the means
```

Great. We can see that our mean of the sampling distribution matches are true mean, just like we expect. Now how do we calculate the sampling error. From Table \@ref(tab:tablea) we can see that the sampling variance is calculated as $\frac{SD^2}{N}$. We know that the sd is approximately 0.988 and we have a sample size of 20 so that means our sampling variance is expected to be approximately `r 0.988^2 / 20` (`0.988^2 / 20`). 

Remember, we are estimating the sampling variance for the mean effect statistic. Given that we have the sampling distribution for the mean estimate we can calculate this directly from the sampling distribution by simply calculating the variance of the distribution. Let's do that:

```{r}
# Calculate the mean sampling variance
var(means)
```

There you have it. When you see the sampling variance equations in Table \@ref(tab:tablea) all you need to remember is that these are shortcuts to doing the simulations we have just done above. 

## **How does meta-analysis work?**

Meta-analysis quantitatively aggregates effect size data collected from existing research and 'controls' for the sampling variability in the process. Why would we want to account for a given study's sampling variance? We want to do this because studies vary greatly in their sample size, and thus their power, to detect effects. As meta-analysts we want to weight effect sizes from studies with higher power more in an analysis. These studies are more likely to be 'correct' and their estimates less biased as a result of sampling variance.

[Marc Lajeunesse](http://lajeunesse.myweb.usf.edu) does a brilliant job explaining the goals and types of models that meta-analysts use. In the following video he describes why weighting in meta-analysis is so important. 

```{r, echo=FALSE}
embed_url("https://www.youtube.com/watch?app=desktop&v=3XkC_jetn-U") %>% use_align("center")
```

We can effectively think of a meta-analysis as a weighted regression model with the weights being the inverse sampling variance for each effect size. Weights are calculated differently depending on the meta-analytic model in question.

## **Meta-analysis with `metafor`!**

We will make use of the `metafor` package [@Viechtbauer2010] for conducting our meta-analyses. It has substantial capabilities. If you're not familiar with `metafor` have a look at Wolfgang's fantastic `UseR` talk below. Having said that, we'll explain in the Rmarkdown document for the workshop what the code is doing and why.

```{r, echo=FALSE}
vembedr::embed_url("https://www.youtube.com/watch?v=IkduL5iRdqo") %>% vembedr::use_align("center")
```

## **References**

<div id="refs"></div>

<br>
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
