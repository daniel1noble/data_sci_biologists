---
title: "Introduction to Meta-analysis"
author: Daniel W.A. Noble
date: '`r Sys.Date()`'
bibliography: ./bib/refs.bib
csl: ./bib/the-journal-of-experimental-biology.csl
output:
  bookdown::html_document2:
    css: style.css
    code_folding: show
    number_sections: no
    toc: yes
    toc_depth: 6
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, tidy = TRUE)
options(digits=2)
```

```{r klippy, echo=FALSE, include=TRUE, message=FALSE, warning=FALSE}
#install.packages("devtools")
remotes::install_github("rlesur/klippy")
klippy::klippy(tooltip_message = 'Click to Copy Code', tooltip_success = 'Done', position = 'right', color = "red")

# Load packages
pacman::p_load(metafor, flextable, tidyverse, orchaRd, pander, mathjaxr, equatags, vembedr)

# To use mathjaxr you need to run equatags::mathjax_install()
```

## **What is meta-analysis and why do we do it?**

> Statistical methods and techniques for aggregating, summarizing, and drawing inferences from collections of studies

Meta-analysis is the gold-standard for research synthesis across all disciplines. We synthesise studies to gain broader insights into the efficacy of treatment effects and / or relationships between variables. In biology, we are also dealing with many different populations and species. As such, we're not just interested in understanding 'what the overall effect' actually is (in fact, we may not even care in many cases), but we are mainly focused on attempting to understand what factors (e.g., biological, methodological) explain variation in effects [@Noble2022; @Lag2010; @Gurevitch2018]. A key part of meta-analysis is quantifying variability in effects across studies, or what is referred to as 'heterogeneity' in meta-analysis [@ODea2021; @NakagawaSantos2012; @Borenstein2019; @Nakagawa2017; @Gurevitch2018]. 

Think about it this way, in a meta-analysis we want to know what the average effect is across studies to tell us what the mean effect in the population is likely to be. We also want to understand why one experiment gives a different effect size (e.g., Cohen's d) than the same experiment done on a different population. 

## **Meta-analysis aggregates / pools effect sizes**

Meta-analysis is a way to synthesise effect size statistics (i.e., a statistic such as Cohen's *d*) using a special type of mixed effects model that accounts for our sampling uncertainty in each effect estimate. Effect size statistics are the data we use to fit models. 

Effect size statistics can vary but they all have a few key things in common. First, the effect size must be comparable across studies. This means that they must be placed on the same scale so that they can be aggregated. Second, we must be able to estimate the sampling variance for a given effect size. As you're well aware the statistic (effect size) we want to synthesise has uncertainty associated with it because of sampling variability -- the standard error around a mean, for example, is sampling error. In meta-analysis we use this sampling error to decrease the uncertainty in our mean effect size. There are lots of different effect sizes out there, some familiar one are in Table \@ref(tab:tablea). 

```{r tablea, echo = FALSE, tab.cap = "**Common effect sizes used throughout meta-analyses and their associated sampling variances. Examples on when they might be used are also provided**. M: mean; SD: standard deviation; N: sample size"}
FitFlextableToPage <- function(ft, pgwidth = 6){

  ft_out <- ft %>% flextable::autofit()

  ft_out <- flextable::width(ft_out, width = dim(ft_out)$widths*pgwidth /(flextable::flextable_dim(ft_out)$widths))
  return(ft_out)
} 

# I'm just listing a bunch here, but we should think about what we want to include. Here's an example of how we can make a table with all equations. Fun! Really useful website here: https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference. Something wrong with character coding when rendering word document. Has to do with the log odds ratio equation. Renders fine in html, but just stops when word
names <- c("Mean",
           "Log Response Ratio, lnRR",
           # leave this for now??? - we introduce this later
           "Standardised Mean Difference, SMD",
           "Zr (Fisher Transformation of \n Correlation Coefficient, r)")
           #"Log Odds Ratio",
           #"")

# the index i is not really applied consistently so I am adding it more (to do)
# we shoudl discuss what kinds of notations we want to use consistely
           
eqns_yi <- c("M",
             # Senior will have a better version but this is good for now
             "\\ln \\left( \\frac{M_{1}}{M_{2}}\\right)",
             "\\frac{\\left( M_{2} - M_{1}\\right)}{SD_{p}}J",
             # we need to spell out SD_p somewhere
             "\\frac{1}{2}log \\left( \\frac{1+r}{1-r}\\right)")
             #"log\\left(\\frac{y_{i2}+0.5}{n_{i2}-y_{i2}+0.5}\\right) ",
             #"- log\\left(\\frac{y_{i1}+0.5}{n_{i1}-y_{i1}+0.5}\\right)")
             
# Just placeholders for now
eqns_vi <- c("\\frac{SD^2}{N}",
             "\\frac{SD_{1}^2} {M^2_{1}N_{1}} + \\frac{SD_{2}^2} {M^2_{2}N_{2}}",
             "\\frac{N_{1} + N_{2}}{N_{1}N_{2}} + \\frac{SMD^2}{2(N_{1} + N_{2})}",
             # see my commenst
             "\\frac{1}{N - 3}")
             #"\\frac{s_{i}^2}{n_{i}}",
             #"")

# Maybe add some examples for each effect size in a new column. SHould help
eg <- c("CTmin, CTmax, LC50, LT50, Metabolic Rate (MR)",
        "Ratio between pollutant exposed treatment (e.g., BPA-exposed group) and control (no pollutant)",
        "Difference in immune response between males and females, performance difference in the presence of stressor compared to absence of stressor",
        "Relationship between sex hormones and immune responses or metabolic rate and behaviour")

table_fin <- data.frame(names, eqns_yi, eqns_vi, eg)

footnotes <- c("Notes: J = 1 - \\frac{3}{4\\left(N_{1} + N_{2} - 2\\right) - 1}",
               "SD_{p} = \\sqrt \\frac{\\left( N_{1}-1 \\right)SD_{1}^2 + \\left( N_{2}-1 \\right)SD_{2}^2}{N_{1} + N_{2} - 2}")

# Note: It is esssential that you be clear that the function 'compose' is exported from the flextable package. That's because purr also has the same function, so it gets messy.
tablea <- flextable(table_fin)                                                                                %>% 
          flextable::compose(j = 1:4, part = "header", value = as_paragraph(as_b(c("Effect Measure", "Definition", "Sampling Variance", "Examples")))) %>% 
          flextable::compose(j = 2:3, value = as_paragraph(as_equation(c( eqns_yi, eqns_vi))))             %>%
          flextable::compose(j = 4, value = as_paragraph(eg))             %>%
          flextable::width(j = 1, 3)                                                                          %>% 
          flextable::width(j = 2, 2) %>% 
          flextable::width(j = 4, 5) %>% 
          flextable::footnote(i = 4, j = 1, value = as_paragraph(as_equation(footnotes)),
               ref_symbols = c("a"),
               part = "body", inline = TRUE) %>% 
          flextable::font(part = "all", fontname = "Times New Roman")
          
FitFlextableToPage(tablea, pgwidth = 8)
```


Many studies in ecology and evolution meta-analyse the mean, but more commonly standardised effect size statistics are used like the log response ratio (big in Ecology), the standardised mean difference (i.e., Cohen's d, Hedges' g -- you already were exposed to Cohen's d in Eric's workshops!), and correlation coefficients. In the next section we're going to dwell on the effect size statistics, and more specifically their sampling variances, a little more to make sure we understand exactly what these equations are saying.

## **Understanding Sampling Variances**

But wait, what do we really mean when we say sampling variance of an effect size statistic? It's all fine that we have equations in Table \@ref(tab:tablea) to calculate this for various statistics, but they also look scary, and I don't actually know what they are really telling me about the statistic in question! 

If that's what your thinking that is totally normal. Don't fret! We're going to try to  breakdown exactly what we mean by sampling variance and show that these equations are nothing more then simple little shortcuts for us to estimate the sampling uncertainty of effect size statistic. 

Let's return to what we learnt at the beginning the semester about sampling distributions. In order to understand sampling variance we need to do a thought experiment. When we collect sample sizes and statistics from a paper to calculate an effect size statistic and it's sampling variance we really need to remember that this is a statistic that is provided from a single sampling event. It's estimated with uncertainty -- potentially lots of uncertainty of the sample sizes are small. 

Now, let's assume that we take that study and turn it into a simple simulation so that we can endlessly repeat the experiment to create a sampling distribution of the statistic -- the variance in that sampling distribution is in fact that sampling variance for the effect statistic. 

Lets apply this thinking to estimating a very simple statistic and it's sampling variance described in Table \@ref(tab:tablea), the mean. To do this, we need to imagine ourselves going out and sampling from a population many, many times; taking the same size of sample each time. Again, we know the population has a 'true' mean, but we are trying to estimate that from a small sample so we know it will be off each time we sample. Enough talking, lets code this scenario. For ease of understanding it's good to apply a loop here just to wrap your head about what's going on.

```{r, meansim, class.source='klippy'}
# First lets set up some parameters. We'll take 20 samples from a population with a mean of 10.8 and a sd of 0.988
  nsim = 10000
     n = 20
  mean = 10.8
    sd = 0.988

# Now, lets create an empty vector to store our sample mean. We'll then loop, sampling 20 from the population described above but we'll take 10000 samples so we can get a fairly accurate estimate of the 'true' mean and it's sampling error
  means <- c()

   for (i in 1:nsim){
       sample <- rnorm(n, mean = mean, sd = sd)
        means <- c(means, mean(sample))
    }

```

Let's now have a look at our sampling distribution of means that we calculated for each of our 10000 samples:

```{r sampldist}
# Now lets plot the sampling distribution. We'll show the real, true mean, 10.8, and the actual mean of the sampling distribution.
ggplot2::ggplot(tibble(means), aes(x = means)) + 
  geom_histogram(binwidth = 0.02) + 
  geom_vline(xintercept = 10.8, col = "red") +                     # Add the true mean in red
  geom_vline(xintercept = mean(means), col = "blue", linetype = 2) # Add the mean of the means
```

Great. We can see that our mean of the sampling distribution matches our true mean, just like we expect. 

Now, how do we calculate the sampling error? From Table \@ref(tab:tablea) we can see that the sampling variance is calculated as $\frac{SD^2}{N}$. We know that the sd is approximately 0.988 and we have a sample size of `n` so that means our sampling variance is expected to be approximately `r 0.988^2 / n` (`0.988^2 / n`). 

Remember, we are estimating the sampling variance for the mean effect statistic in our simulation. Given that we have the sampling distribution for the mean estimate we can calculate this directly from the sampling distribution by simply calculating the variance of the distribution. Let's do that:

```{r}
# Calculate the mean sampling variance

tab1 <- data.frame(Approach = c("Analytical", "Simulation"), `Sampling Variance` = c(0.988^2 / n, var(means)), check.names = FALSE)
flextable(tab1)

```

There you have it. When you see the sampling variance equations in Table \@ref(tab:tablea) all you need to remember is that these are shortcuts to doing the simulations we have just done above under the specific sampling scenario that we created in a study. 

I'm going to assume here that you're super critical and you don't believe me (good!) so lets do this again with a more complicated effect size statistic, the log response ratio. 

```{r, meansimlnrr, class.source='klippy'}
# First, lets set up some parameters. We'll take 20 samples from two populations that we are interested in contrasting (say a control and treatment group). The control group has a true mean of 10.8 and a sd of 0.988, whereas the treatment group has a true mean of 5.5 with a sd of 1.2. What that means is, effectively, the log response ratio is: log(10.8/5.5), or 0.67, because we are interesting in contrasting the two groups
   nsim = 10000
      n = 20
  mean1 = 10.8
    sd1 = 0.988
  mean2 = 5.5
    sd2 = 1.2

# Now, lets create an empty vector to store our log response ratio effect size statistic that we calculate from our samples. We'll then loop, sampling 20 from both groups and calculating the log response ratio, and we'll do this 10000 samples so we can get a fairly accurate estimate of the log response ratio statistic.
  lnrr <- c()

   for (i in 1:nsim){
       sample1 <- rnorm(n, mean = mean1, sd = sd1)
       sample2 <- rnorm(n, mean = mean2, sd = sd2)
         lnrr <- c(lnrr, log(mean(sample1)/ mean(sample2)))
    }

```


```{r sampldistlnrr}
# Now lets plot the sampling distribution. We'll show the real, true mean, 0.67, and the actual mean of the sampling distribution.
ggplot2::ggplot(tibble(lnrr), aes(x = lnrr)) + 
  geom_histogram(binwidth = 0.02) +
  geom_vline(xintercept = log(10.8 / 5.5), col = "red") +                     # Add the true mean in red
  geom_vline(xintercept = mean(lnrr), col = "blue", linetype = 2) + # Add the mean of the means
  labs(x = "log Response Ratio (lnRR)")
```

Table \@ref(tab:tablea) above tells us what the sampling variance should be for this situation. We can calculate it ourselves from the equation and see if it matches the variance of the distribution:

```{r}
# We know the 'true' values so we can use them to calculate what the sampling variance for this specific situation should be. 
     sv_lnrr <- (sd1^2 / (mean1^2*n)) + (sd2^2 / (mean2^2*n))

# We just did a simple simulation of the situation where we only calculate the effect size statistic, now lets see what the sampling variance is of the lnRR sampling distribution that we calculated.
samp_dist_lnRR <- var(lnrr)

# Create a table to contrast these a little more clearly
tab <- data.frame(Approach = c("Analytical", "Simulation"), `Sampling Variance` = c(sv_lnrr,samp_dist_lnRR), check.names = FALSE)
flextable(tab)
```

What I recommend doing now is to play around with `n`. Change it and rerun the code. How does the sampling variance change?


Let's now try Zr!

```{r}

# Lets set up the simulated experiment. We'll again conduct 10000 simulations, or experiments, where we sample 20 individuals from a population and measure 2 traits on those individuals that have a correlation of 0.70. 
     nsim = 10000
        n = 20
        r = 0.70
covMatrix = matrix(c(1, r, r, 1), nrow = 2, ncol = 2)

# Create an empty vector to store our simulated Zr values.
  Zr <- c()

# Run our simulation
 for(i in 1:nsim){
    sim_r <- MASS::mvrnorm(n = n, mu = c(0,0), Sigma = covMatrix)
      cor <- cor(sim_r)[1,2]
      Zr_tmp <- 0.5*log((1 + cor) / (1 - cor))
      Zr <- c(Zr, Zr_tmp)
  }

```


```{r sampldistZr}
# Now lets plot the sampling distribution. 
ggplot2::ggplot(tibble(Zr), aes(x = Zr)) + 
  geom_histogram(binwidth = 0.02) +
  geom_vline(xintercept = 0.5*log((1 + r) / (1 - r)), col = "red") +                     # Add the true mean in red
  geom_vline(xintercept = mean(Zr), col = "blue", linetype = 2) + # Add the mean of the means
  labs(x = "Z-transformed Correlation Coefficient (Zr)")
```

```{r}
# We know the 'true' values so we can use them to calculate what the sampling variance for this specific situation should be. 
     sv_Zr <- 1 / (n - 3)

# We just did a simple simulation of the situation where we only calculate the effect size statistic, now lets see what the sampling variance is of the Zr sampling distribution that we calculated.
samp_dist_Zr <- var(Zr)

# Again, let's create a table to contrast these a little more clearly
tab <- data.frame(Approach = c("Analytical", "Simulation"), `Sampling Variance` = c(sv_Zr,samp_dist_Zr), check.names = FALSE)
flextable(tab)
```

## **How does meta-analysis work?**

Meta-analysis quantitatively aggregates effect size data collected from existing research and 'controls' for the sampling variability in the process. Why would we want to account for a given study's sampling variance? We want to do this because studies vary greatly in their sample size, and thus their power, to detect effects. As meta-analysts we want to weight effect sizes from studies with higher power more in an analysis. These studies are more likely to be 'correct' and their estimates less biased as a result of sampling variance.

[Marc Lajeunesse](http://lajeunesse.myweb.usf.edu) does a brilliant job explaining the goals and types of models that meta-analysts use. In the following video he describes why weighting in meta-analysis is so important. 

```{r, echo=FALSE}
embed_url("https://www.youtube.com/watch?app=desktop&v=3XkC_jetn-U") %>% use_align("center")
```

We can effectively think of a meta-analysis as a weighted regression model with the weights being the inverse sampling variance for each effect size. Weights are calculated differently depending on the meta-analytic model in question.

## **Meta-analysis with `metafor`!**

We will make use of the `metafor` package [@Viechtbauer2010] for conducting our meta-analyses. It has substantial capabilities. If you're not familiar with `metafor` have a look at Wolfgang's fantastic `UseR` talk below. Having said that, we'll explain in the Rmarkdown document for the workshop what the code is doing and why.

```{r, echo=FALSE}
vembedr::embed_url("https://www.youtube.com/watch?v=IkduL5iRdqo") %>% vembedr::use_align("center")
```

## **References**

<div id="refs"></div>

<br>
<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>
